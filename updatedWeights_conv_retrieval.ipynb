{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "updatedWeights_conv_retrieval.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPZogNdNSprmXKfRe+WcYbi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hembrickad/HoloAAC/blob/swati_files/updatedWeights_conv_retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TdarZwuUZLz",
        "outputId": "a8016d29-69df-4d35-c258-64e13cfd598f"
      },
      "source": [
        "# for any questions about this code, please make sure to contact the owner: Swati Rampalli (rampa009@umn.edu)\n",
        "# please see the \"NOTE:\" for potential issues and suggestions with the code \n",
        "\n",
        "# there are multiple dependencies you need to install to run this conversation retrieval script\n",
        "# NOTE: spaCY took a painstakingly long time to download locally, so I used the Colab document for now\n",
        "# spaCY is used primarily because of its comprehensive stop word removal, but if you find it too unfeasile\n",
        "# to download locally, you can opt for a better stopword removal package \n",
        "\n",
        "# NOTE: I manually created this dataset myself, so it is only 100 sentences. For further testing purposes, you might \n",
        "# want to add more sentences. An initial problem I had was that I was getting a lot of the same weights for words, \n",
        "# which really didn't signify anything important. This was because the frequency of my words in the dataset were VERY\n",
        "# similar so when put into the log function that Tfidf does to calculate the weights, the weights turned out to be \n",
        "# basically the same. This could probably be solved (or at least worked around) with a significantly larger dataset, but\n",
        "# make sure that the frequency of non-stopwords significantly vary. \n",
        "\n",
        "# NOTE: The software that we're using to connect Unity and Python is Upython, which requires a local python file to run. This\n",
        "# is a google colab document (I used this becuase of spacy, see above). You need to either download spacy locally or use \n",
        "# a different stopword removal package\n",
        "\n",
        "# NOTE: If this python script is run locally, you might need to put the path to where your python packages are downloaded\n",
        "# at the front of the file (you can do this with sys.path(location to python packages)). Otherwise, IDLE will not be able \n",
        "# to see where the \"nltk\" package is stored. This usually happened because of conflicting paths between Anaconda and IDLE. \n",
        "\n",
        "\n",
        "!pip install spacy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.6.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVeJ5ioXUj8I",
        "outputId": "b0835d7d-f5f0-46b5-c66b-e6d1dc197500"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YMtmIE1Urua",
        "outputId": "ee8d2ac7-6b75-4673-a900-3e1191d94736"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rz8W-CcEVTvB",
        "outputId": "209d8d90-e70a-40f5-e455-005afd280dcf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jM26LVgOUaIv"
      },
      "source": [
        "import nltk\n",
        "import random\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from heapq import nlargest\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "sw_nltk = stopwords.words('english')\n",
        "import spacy\n",
        "en = spacy.load('en_core_web_sm')\n",
        "sw_spacy = en.Defaults.stop_words\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# this function was used to determine whether the word is an adverb, noun, etc. for lemmatization purposes\n",
        "# refer to this link for info on lemmatization and how it differs from stemming: https://www.machinelearningplus.com/nlp/lemmatization-examples-python/ \n",
        "# NOTE: stemming doesn't retain the english word AND doesn't account for the context/meaning, which is why I used a lemmatizer instead\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "# this \"keyword\" is the object that was detected like \"apple, milk, etc.\" and is initially used\n",
        "# to just gather ALL sentences containing the keyword - we want this initially so we can better \n",
        "# refine that list with additional keywords later on \n",
        "keyword = \"milk\"\n",
        "\n",
        "return_sentences = []\n",
        "\n",
        "# mounting the google drive is specific to the colab document, if this file is run locally, then this should be \n",
        "# replaced by the local paths \n",
        "sentences_file = open(\"/content/gdrive/My Drive/Colab Notebooks/grocery_store_sentences.txt\", \"r\", encoding='utf-8-sig')\n",
        "sentences_list = sentences_file.readlines()\n",
        "stripped_sentences_list = []\n",
        "returned_sentences_list = []\n",
        "stemmed_sentence_list = []\n",
        "\n",
        "punctuations=\"?:!.,;\"\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "\n",
        "for sentence in sentences_list:\n",
        "    strip = sentence.strip()\n",
        "    if keyword in strip:\n",
        "      stripped_sentences_list.append(strip)\n",
        "\n",
        "# I used this example list for testing purposes because I was getting the same weights \n",
        "# for the words, indicating that words like \"nonfat\" and \"bag\" had the same frequency, \n",
        "# even though \"bag\" was used a lot more \n",
        "\n",
        "#stripped_sentences_list = ['Can I weigh this apple', 'Can I weigh this avocado',\n",
        "                            #'Can I weigh this banana',\n",
        "                           # 'Can I weigh this orange',\n",
        "                          #  'Can I weigh this tomato',\n",
        "                           # 'Can I weigh this ketchup',\n",
        "                           # 'Can I weigh this soda',\n",
        "                           # 'Can I weigh this drink',\n",
        "                          #  'Can I buy this mango',\n",
        "                          #  'Can I buy this water'\n",
        "                          #  ]\n",
        "\n",
        "# for preprocessing purposes, I used the WordNet lemmatizer and spaCY stopword removal which will boil \n",
        "# the sentence down to its root words (i.e. playing turns into play) and remove words that have no meaning (like this, and) \n",
        "\n",
        "for sentence in stripped_sentences_list:\n",
        "    words_to_join = []\n",
        "    possible_adj = []\n",
        "    #stripped_sentence = sentence.strip()\n",
        "    stemmed_list = nltk.word_tokenize(sentence)\n",
        "    for word in stemmed_list:\n",
        "      # removing ALL punctuation because this isn't recognized by the lemmatizer \n",
        "        if word in punctuations:\n",
        "            stemmed_list.remove(word)\n",
        "    tokens_without_stop = [word for word in stemmed_list if not word.lower() in sw_spacy]\n",
        "\n",
        "# this step is a little extra becuase I was having problems with lemmatization earlier since there were a bunch\n",
        "# of different possible pertainyms - refer to this site to know what I mean: http://wordnetweb.princeton.edu/perl/webwn?s=separately\n",
        "# I basically narrowed this down by choosing the one that had the same 3-letter root word as the word of interest,\n",
        "# so \"separately\" would have multiple different pertainyms - \"individual\", \"separate\", \"single\" and \"separate\" is the one we want \n",
        "\n",
        "    for word in tokens_without_stop:\n",
        "        if get_wordnet_pos(word) == \"r\" and str(wordnet.synsets(word)[0])[:-5][-1] == 'r':\n",
        "            for ss in wordnet.synsets(word):\n",
        "                for lemmas in ss.lemmas(): # all possible lemmas\n",
        "                    for ps in lemmas.pertainyms(): # all possible pertainyms\n",
        "                        possible_adj.append(ps.name())\n",
        "            for adj in possible_adj:\n",
        "                if adj[0:3] == word[0:3]:\n",
        "                    words_to_join.append(adj)\n",
        "        else:\n",
        "            words_to_join.append(lemmatizer.lemmatize(word, get_wordnet_pos(word)))\n",
        "\n",
        "    lemmatized_output = ' '.join(words_to_join)        \n",
        "    #lemmatized_output = ' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in tokens_without_stop])\n",
        "    stemmed_sentence_list.append(lemmatized_output)\n",
        "    \n",
        "\n",
        "#vectorizer = TfidfVectorizer()\n",
        "#X = vectorizer.fit(stemmed_sentence_list)\n",
        "#print(vectorizer.vocabulary_)\n",
        "##\n",
        "#### figure out how to strip all punctuation - maybe beforehand, when\n",
        "#### reading the sentences from the file?\n",
        "##\n",
        "#### only 100 sentence dataset - maybe too small to accurately represent what\n",
        "#### the proper distinguishing words are for each class?\n",
        "##\n",
        "##\n",
        "###instantiate CountVectorizer() \n",
        "cv=CountVectorizer(token_pattern='(?u)\\\\b\\\\w+\\\\b') \n",
        "\n",
        "# this steps generates word counts for each of the words in the sentences and assigns a weight to each word\n",
        "# NOTE: words more frequent have smaller weights compared to words less frequent \n",
        "\n",
        "word_count_vector=cv.fit_transform(stemmed_sentence_list)\n",
        "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \n",
        "tfidf_transformer.fit(word_count_vector)\n",
        "df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(),columns=[\"idf_weights\"]) \n",
        "\n",
        "# gets the top 5 words with the highest weights, i.e. the most \"distinguishing\" words\n",
        "\n",
        "df_idf.sort_values(by=['idf_weights'])\n",
        "top_words = list(df_idf.tail(5).index)\n",
        "\n",
        "\n",
        "# this is where we refine our initial list of sentences such that if the user wants to select additional keywords after the \n",
        "# original object class, they can do so. \n",
        "# I also assigned \"fake\" click counts from 1-5, designated by the random library to simulate user selection history as another\n",
        "# parameter for ranking. For example, if three sentences popped up, I would further refine it with the one with the highest \n",
        "# click counts\n",
        "\n",
        "additional_keywords = \"bag\"\n",
        "refined_sentence_dict = {}\n",
        "stripped_dict = {}\n",
        "\n",
        "# a simulated list of all the sentences with the object name and their \"click counts\"\n",
        "# this is intended to show how the user's previous selection history can be accounted for \n",
        "for item in stripped_sentences_list:\n",
        "  stripped_dict[item] = random.randint(1,5)\n",
        "\n",
        "# the narrowed list of sentences based on the additional keyword(s) provided\n",
        "for sentence in stripped_dict:\n",
        "  if additional_keywords in sentence:\n",
        "    refined_sentence_dict[sentence] = stripped_dict[sentence]\n",
        "\n",
        "# sentence with the highest click count out of the narrowed list of sentences\n",
        "max_key = max(refined_sentence_dict, key=refined_sentence_dict.get)\n",
        "\n",
        "# to view the list of weights, type \"df_idf\"\n",
        "# to view the list of sentences JUST with the object name and their click counts, type \"stripped_dict\"\n",
        "# to view the list of refined sentences BASED on the additional keyword(s), type \"refined_sentence_dict\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HznBRT3bbjre"
      },
      "source": [
        "##unique_sentence_weights = {}\n",
        "###for all the sentences containing the keyword                \n",
        "##for basic_sent in returned_sentences_list:\n",
        "##    unique_sentence_weights[basic_sent] = 0\n",
        "##    #compute the sum of the idfs for a sentence with no duplicate words\n",
        "##    #make the sentence unique first (remove duplicate words like \"the\")\n",
        "##    basic_sent_split = basic_sent.lower().strip(\"?.’\").split(\" \")\n",
        "##    for word in basic_sent_split:\n",
        "##        if word not in unique_sentence_weights:\n",
        "##            unique_sentence_weights[basic_sent] = unique_sentence_weights[basic_sent]+ float(df_idf.loc[[word.lower()], 'idf_weights'][word.lower()])\n",
        "##        \n",
        "##highest_sen = nlargest(4, unique_sentence_weights, key = unique_sentence_weights.get)              \n",
        "##     \n",
        "##\n",
        "##sentences_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "7M-qlETIUhv3",
        "outputId": "8959dbcb-349e-4a88-defa-9a70b18a4743"
      },
      "source": [
        "df_idf"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>idf_weights</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.609438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.609438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bag</th>\n",
              "      <td>2.321756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>double</th>\n",
              "      <td>3.014903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>expire</th>\n",
              "      <td>3.014903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>find</th>\n",
              "      <td>3.014903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gallon</th>\n",
              "      <td>2.098612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>milk</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nonfat</th>\n",
              "      <td>2.321756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>organic</th>\n",
              "      <td>3.014903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>price</th>\n",
              "      <td>2.098612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>separate</th>\n",
              "      <td>2.609438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>skim</th>\n",
              "      <td>2.321756</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          idf_weights\n",
              "1            2.609438\n",
              "2            2.609438\n",
              "bag          2.321756\n",
              "double       3.014903\n",
              "expire       3.014903\n",
              "find         3.014903\n",
              "gallon       2.098612\n",
              "milk         1.000000\n",
              "nonfat       2.321756\n",
              "organic      3.014903\n",
              "price        2.098612\n",
              "separate     2.609438\n",
              "skim         2.321756"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnZhbpoPyjyY",
        "outputId": "8d1fcbc9-1ce3-4b14-a789-02fb5647d570"
      },
      "source": [
        "stripped_dict"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Can you put the 2% and 1% milk in separate bags?': 1,\n",
              " 'Can you put the whole milk and skim milk in separate bags?': 3,\n",
              " 'Could you double bag the milk gallons?': 2,\n",
              " 'Do you have 2% milk?': 4,\n",
              " 'Do you have more of the 1% milk?': 5,\n",
              " 'Do you have organic milk?': 1,\n",
              " 'Do you have skim milk?': 3,\n",
              " 'What is the price of a milk gallon?': 4,\n",
              " 'What is the price of this milk gallon?': 2,\n",
              " 'What is the price of this nonfat milk gallon?': 1,\n",
              " 'What is the price of this skim milk?': 5,\n",
              " 'When does this nonfat milk expire?': 3,\n",
              " 'Where can I find nonfat milk?': 1,\n",
              " 'Where is the whole milk?': 3}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0AuOg5LsxDJ",
        "outputId": "ba3b20de-92e6-4f11-b5b4-6fcc29ebcee8"
      },
      "source": [
        "refined_sentence_dict"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Can you put the 2% and 1% milk in separate bags?': 1,\n",
              " 'Can you put the whole milk and skim milk in separate bags?': 3,\n",
              " 'Could you double bag the milk gallons?': 2}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLLb_tBcWCcW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3ed22277-8049-4a38-e3db-898fe0b31c33"
      },
      "source": [
        "max_key"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Can you put the whole milk and skim milk in separate bags?'"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}